---
title: First steps into data science
author: Thinkful
team: grading
time: 30 minutes
uuid: e06d8c0e-f542-44dd-bb2b-29cfde3614de
timeHours: 0.5
---

The term *data science* originated at the turn of the 21st century through a series of papers and presentations by American computer scientist [William S. Cleveland](http://www.stat.purdue.edu/~wsc/). Writing primarily for an academic audience, Cleveland noted new methodologies that combined computer science and classical statistics. 

In the late 2000s, DJ Patil of LinkedIn and Jeff Hammerbacher of Facebook began to create teams of what they called ["data scientists"](http://radar.oreilly.com/2011/09/building-data-science-teams.html) to derive business value out of the extremely large amount of data being generated by their websites. Patil went on to coauthor an article in the *Harvard Business Review* with the now-infamous headline, "[Data Scientist: The Sexiest Job of the 21st Century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)."

Given that headline, there's a good amount of marketing and hype around what is known today as data science. Some of this came in the wake of *Big Data,* a term which, according to [Google Trends](https://trends.google.com/trends/explore?date=all&geo=US&q=big%20data), appears to be losing popularity. It's true that an unprecedented [amount of data](https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/) is generated today. Data science as it is today would not be the same without this "raw fuel" of data. 

What makes data science unique, however, is not just the huge amounts of data used, but the wide-ranging scope of disciplines it operates under. If there's one constant in the job description of the data scientist, it's the requirement to know a lot and to keep learning. After all, the job title didn't even exist twenty years ago. There's no state licensing agency or union for data science. Anyone can *call* themselves a data scientist. So, given the novelty and the scope of the field, how is data science defined? Who is a data scientist?  


## The best definition is a visual one

Rather than apply a one-size-fits-all definition for data science, you should heed a classic data science motto: "When in doubt, visualize it."

Take a look below at the famous "Data science Venn diagram," introduced [in a post](http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram) by Drew Conway. This is the first of [many attempts](https://www.kdnuggets.com/2016/10/battle-data-science-venn-diagrams.html) by data scientists to visualize what data science is. Conway uses the Venn diagram to highlight that data science is inherently multidisciplinary. As data science combines traditional mathematics and statistics with computer science and subject matter expertise, there's no one fixed path to becoming a data scientist. 

Even within the center of the Venn diagram, some data scientists may be more comfortable with one discipline than another. The objective, rather, is to become versatile enough in each of the three major areas to float nimbly across them, while avoiding the "danger zone," which you'll learn more about later. 

![Drew Conway’s data science Venn diagram.](ds_venn_diagram.png)

### Hacking skills

Today, the term *hacking* carries negative connotations of data breaches and phishing, but [it](https://haenfler.sites.grinnell.edu/subcultures-and-scenes/hacker-subculture/) was originally used in a spirit of playfulness and creativity in overcoming the limitations of software to achieve novel outcomes. Data scientists have built robust statistical models using data sources that statisticians just a few decades ago would have never imagined. They've done this through *hacking*. 

Nearly any data in digital format is fair game for data science. This course will cover working with data ranging from CSV files, to relational databases, to making API calls and scraping data from a website. Data scientists can even use data sources like images and audio. Regardless of the medium, the challenge—and frustration—of data science is the process of converting this data into something that can be analyzed. 

*The New York Times* reported that data scientists spend up to [80 percent of their time](https://www.nytimes.com/2014/08/18/technology/for-big-data-scientists-hurdle-to-insights-is-janitor-work.html) collecting and preparing data for analysis—a task known as *data wrangling* or *data munging.* While this process can provide valuable insights about a dataset, it is *not* the analysis itself. A convoluted data wrangling process can halt a data science project in its tracks, leaving a winding trail of inconsistent, error-prone data behind it. 

A strong command of a programming language, such as Python, helps data scientists automate this routine element of data science. Strong hacking skills ensure that this data can be collected, free of error, time and again. 

### Math and statistics knowledge

This part of the Venn diagram above is the domain of classical mathematics and statistics: think of concepts like the central limit theorem, probability distributions, and Student's *t*-test. While foundational to data science, classical math and statistics operate differently than modern machine learning.

Statistics emphasizes a formalization of relationships between variables. Data is used to develop a theory based on hypothesis testing. These methods often focus on inferring characteristics of a larger population from a small sample of data. Tests used often have explicit assumptions about how the data should "look"; for example, normality or linearity. This is a *rules-driven* system for developing insights from data.

### Substantive expertise

Data scientists are celebrated geeks, but they must also be shrewd business thinkers and strategists. While building models and automating tasks make up the data scientist's day-to-day activities, the big picture objective is to provide value to the organization or clients. 

This means that data scientists must also be able to develop relevant research questions and hypotheses. Data science should be used as a lens for viewing real-world problems. The objective is not to focus on the mechanics of that lens, but to use it to see the world more clearly. 

Data scientists should also understand the landscape of their industry: what are the financial drivers? The competitive landscape? What are the key opportunities for the organization? What unique data is available? The ability to make decisions about data collection and modeling in the context of a given industry ensures that what seems like a successful data science project won't be in vain.

### Machine learning: Hacking + math

Machine learning combines classical statistics with the creative, pragmatic approach of hacking. If theory development is the objective of classical statistics, predictive accuracy and real-world usefulness is the objective of machine learning. Rather than analyzing data as an explicit, rules-driven system informed by the researcher, machine learning attempts to build pattern-based systems that can learn from data, rather than explicitly programmed instructions. 

Data collection in early statistics was often costly and time-consuming, as the data was gathered manually by researchers. Instead, data scientists usually gather what data is already available, and that often means a [*lot*](https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/) of data. That's a good thing, because machine learning systems can require a lot of data to predict accurately. More data in itself, however, does not make for a better model; the old saying, "garbage in, garbage out" comes to mind. This is where machine learning meshes with classical statistics: in diagnosing models that don't work, and validating those that do.  


### Supervised and unsupervised machine learning

There is a *wide* range of techniques of machine learning tools. The rules of which model to use are generally less restrictive than classical statistics, although some machine learning models do have assumptions about the input data. As a whole, machine learning algorithms can be described as either *supervised* or *unsupervised.*

Supervised learning attempts to predict a known outcome variable, such as dollars, weight, or color.  The desired result of the model is known, or *labeled*. This process is known as supervised learning because the known, labeled data acts as a teacher by iteratively correcting the model's predictions. Linear regression is an example of supervised learning: knowing *Y* and training a model of *X*s to predict it. 

In unsupervised learning, there is no known outcome variable to predict. Instead, the goal is to learn about the underlying structure or distribution of the data. In unsupervised learning, there is no "right answer," so there is no teacher. Clustering is an example of unsupervised learning: trying to find an inherent grouping of *X*s based on shared qualities. 


### Traditional research: Expertise + statistics

Traditional research, usually found in academia, can be described as a combination of substantive expertise and traditional statistics. Here, data is used as evidence to test hypotheses and advance a field of research.

Most data scientists operate in the private sector, where the objective of research is not necessarily to develop the body of science for its own sake (or at least to author publications) but instead to achieve some business objective. This means that data scientists aren't just technicians but are also consultants and advocates within their organization.

This intersection of data science with business communication is where data science is likely to find similarities with data analytics and business intelligence. Compelling visualizations and data storytelling are used to develop recommendations to influencers outside the data science team. Dashboards keep operations data-informed. Having a foothold in user experience design and cognitive science can help the data scientist translate their findings into an actionable, approachable business agenda. 

### Danger zone: Hacking + expertise

What makes this combination "dangerous"? In some ways, the combination of hacking and expertise is the dark side of machine learning: you know what objectives matter, and you know how to get data, so you simply feed data into Python until an interesting result is found. Not only is this like a "black box," in that the processes behind the data analysis are unclear, but it's also an *unreliable* black box! 

While machine learning approaches problem solving differently than classical statistics, statistical foundations are essential for validating and stress testing the results of such hacking. It's not enough just to get promising numbers from the model—even numbers that you can impress executives with.  As data scientists John Mount and Nina Zumel write: "The worst possible modeling outcome is not failing to find a good model. The worst possible modeling outcome is thinking you have a good model when you don't." 

## Assignment

Profile yourself with respect to the data science Venn diagram. Consider the following topics when making your profile:


- Computer science
- Software engineering
- Statistics
- Machine learning
- Domain expertise
- Presentation and communication skills
- Data visualization

Based on this skill set, where do you find yourself on the Venn diagram? What areas are your strengths? What areas do you want to grow in? Write a few sentences below answering these questions.


