---
title: Feature engineering III
author: Thinkful
team: grading
time: 200 minutes
uuid: c5057878-fbf0-4394-ae77-a5a39ff2500d
timeHours: 3.3333333333333335
---

In the previous checkpoints, you learned about two language modeling methods, BoW and TF-IDF, that convert text data into numerical form. These methods are based on counting the occurrences of the words in the documents. Here, you'll learn about another approach for language modeling: *word2vec*. This approach has revolutionized NLP applications since its introduction in 2013. Originally developed by a team at Google, word2vec is one of the most popular feature-generation methods, and it has found high applicability in the industry.

<jupyter notebook-name="6.feature_engineering_3" course-code="DSBC"></jupyter>

For a screencast demo of the techniques covered here, check out the below video.

<iframe id="kaltura_player_1604711770" src="https://cdnapisec.kaltura.com/p/2315191/sp/231519100/embedIframeJs/uiconf_id/45331192/partner_id/2315191?iframeembed=true&playerId=kaltura_player_1604711770&entry_id=1_ms3qwi9h" width="100%" height="500" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" frameborder="0"></iframe>

## Assignments

Submit your solutions to the following tasks as a link to your Jupyter Notebook on GitHub.

1. Train your own word2vec representations, as you did in the first example in this checkpoint. However, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again. Can you wrangle any improvements?

Submit your work below. You can also take a look at this [example solution](https://drive.google.com/file/d/1Y1Mb3pORftlmncuy1qrR8BTquM2lw4B3/view?usp=sharing).
